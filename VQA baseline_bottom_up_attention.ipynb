{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/datasets/ee285f-public/VQA2017/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check cuda availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract image and text features from pretrained models and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "url = 'https://imagecaption.blob.core.windows.net/imagecaption/trainval_36.zip'\n",
    "urllib.request.urlretrieve(url, './data/trainval_36.zip')\n",
    "with ZipFile('./data/trainval_36.zip', 'r') as zip:\n",
    "    zip.extractall(path = './data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "urllib.request.urlretrieve(url, './data/glove.6B.zip')\n",
    "\n",
    "with ZipFile('./data/glove.6B.zip', 'r') as zip:\n",
    "    \n",
    "    zip.extractall(path = './data/glove')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create dictionaries for text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary dumped to ./data/dictionary.pkl\n",
      "loading dictionary from ./data/dictionary.pkl\n",
      "embedding dim is 300\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "from dataset import Dictionary\n",
    "\n",
    "\n",
    "def create_dictionary(dataroot):\n",
    "    dictionary = Dictionary()\n",
    "    questions = []\n",
    "    files = [\n",
    "        'v2_OpenEnded_mscoco_train2014_questions.json',\n",
    "        'v2_OpenEnded_mscoco_val2014_questions.json',\n",
    "        'v2_OpenEnded_mscoco_test2015_questions.json',\n",
    "        'v2_OpenEnded_mscoco_test-dev2015_questions.json'\n",
    "    ]\n",
    "    for path in files:\n",
    "        question_path = os.path.join(dataroot, path)\n",
    "        qs = json.load(open(question_path))['questions']\n",
    "        for q in qs:\n",
    "            dictionary.tokenize(q['question'], True)\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def create_glove_embedding_init(idx2word, glove_file):\n",
    "    word2emb = {}\n",
    "    with open(glove_file, 'r', encoding = 'utf-8') as f:\n",
    "        entries = f.readlines()\n",
    "    emb_dim = len(entries[0].split(' ')) - 1\n",
    "    print('embedding dim is %d' % emb_dim)\n",
    "    weights = np.zeros((len(idx2word), emb_dim), dtype=np.float32)\n",
    "\n",
    "    for entry in entries:\n",
    "        #print(entry)\n",
    "        vals = entry.split(' ')\n",
    "        word = vals[0]\n",
    "        vals = vals[1:]\n",
    "        word2emb[word] = np.array(vals)\n",
    "    for idx, word in enumerate(idx2word):\n",
    "        if word not in word2emb:\n",
    "            continue\n",
    "        weights[idx] = word2emb[word]\n",
    "    return weights, word2emb\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    d = create_dictionary(root_dir)\n",
    "    d.dump_to_file('./data/dictionary.pkl')\n",
    "\n",
    "    d = Dictionary.load_from_file('./data/dictionary.pkl')\n",
    "    emb_dim = 300\n",
    "    glove_file = 'data/glove/glove.6B.%dd.txt' % emb_dim\n",
    "    weights, word2emb = create_glove_embedding_init(d.idx2word, glove_file)\n",
    "    np.save('data/glove6b_init_%dd.npy' % emb_dim, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute_softscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import _pickle as cPickle\n",
    "\n",
    "from dataset import Dictionary\n",
    "import utils\n",
    "\n",
    "\n",
    "contractions = {\n",
    "    \"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\":\n",
    "    \"could've\", \"couldnt\": \"couldn't\", \"couldn'tve\": \"couldn't've\",\n",
    "    \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\", \"doesnt\":\n",
    "    \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\", \"hadnt've\":\n",
    "    \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\":\n",
    "    \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\", \"he'dve\":\n",
    "    \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\",\n",
    "    \"hows\": \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", \"Im\":\n",
    "    \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\":\n",
    "    \"it'd've\", \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\",\n",
    "    \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\":\n",
    "    \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\",\n",
    "    \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\",\n",
    "    \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\",\n",
    "    \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\":\n",
    "    \"'ow's'at\", \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\":\n",
    "    \"she'd've\", \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\":\n",
    "    \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\":\n",
    "    \"shouldn't've\", \"somebody'd\": \"somebodyd\", \"somebodyd've\":\n",
    "    \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", \"somebodyll\":\n",
    "    \"somebody'll\", \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\",\n",
    "    \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\",\n",
    "    \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\":\n",
    "    \"something'd\", \"somethingd've\": \"something'd've\", \"something'dve\":\n",
    "    \"something'd've\", \"somethingll\": \"something'll\", \"thats\":\n",
    "    \"that's\", \"thered\": \"there'd\", \"thered've\": \"there'd've\",\n",
    "    \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\":\n",
    "    \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\", \"they'dve\":\n",
    "    \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\":\n",
    "    \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\", \"wed've\":\n",
    "    \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\":\n",
    "    \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\", \"whats\":\n",
    "    \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\":\n",
    "    \"where'd\", \"wheres\": \"where's\", \"whereve\": \"where've\", \"whod\":\n",
    "    \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\":\n",
    "    \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\",\n",
    "    \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\":\n",
    "    \"would've\", \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\",\n",
    "    \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\":\n",
    "    \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\",\n",
    "    \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\":\n",
    "    \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\", \"youll\":\n",
    "    \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"\n",
    "}\n",
    "\n",
    "manual_map = { 'none': '0',\n",
    "              'zero': '0',\n",
    "              'one': '1',\n",
    "              'two': '2',\n",
    "              'three': '3',\n",
    "              'four': '4',\n",
    "              'five': '5',\n",
    "              'six': '6',\n",
    "              'seven': '7',\n",
    "              'eight': '8',\n",
    "               'nine': '9',\n",
    "              'ten': '10'}\n",
    "articles = ['a', 'an', 'the']\n",
    "period_strip = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n",
    "comma_strip = re.compile(\"(\\d)(\\,)(\\d)\")\n",
    "punct = [';', r\"/\", '[', ']', '\"', '{', '}',\n",
    "                '(', ')', '=', '+', '\\\\', '_', '-',\n",
    "                '>', '<', '@', '`', ',', '?', '!']\n",
    "\n",
    "\n",
    "def get_score(occurences):\n",
    "    if occurences == 0:\n",
    "        return 0\n",
    "    elif occurences == 1:\n",
    "        return 0.3\n",
    "    elif occurences == 2:\n",
    "        return 0.6\n",
    "    elif occurences == 3:\n",
    "        return 0.9\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def process_punctuation(inText):\n",
    "    outText = inText\n",
    "    for p in punct:\n",
    "        if (p + ' ' in inText or ' ' + p in inText) \\\n",
    "           or (re.search(comma_strip, inText) != None):\n",
    "            outText = outText.replace(p, '')\n",
    "        else:\n",
    "            outText = outText.replace(p, ' ')\n",
    "    outText = period_strip.sub(\"\", outText, re.UNICODE)\n",
    "    return outText\n",
    "\n",
    "\n",
    "def process_digit_article(inText):\n",
    "    outText = []\n",
    "    tempText = inText.lower().split()\n",
    "    for word in tempText:\n",
    "        word = manual_map.setdefault(word, word)\n",
    "        if word not in articles:\n",
    "            outText.append(word)\n",
    "        else:\n",
    "            pass\n",
    "    for wordId, word in enumerate(outText):\n",
    "        if word in contractions:\n",
    "            outText[wordId] = contractions[word]\n",
    "    outText = ' '.join(outText)\n",
    "    return outText\n",
    "\n",
    "\n",
    "def multiple_replace(text, wordDict):\n",
    "    for key in wordDict:\n",
    "        text = text.replace(key, wordDict[key])\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_answer(answer):\n",
    "    answer = process_digit_article(process_punctuation(answer))\n",
    "    answer = answer.replace(',', '')\n",
    "    return answer\n",
    "\n",
    "\n",
    "def filter_answers(answers_dset, min_occurence):\n",
    "    \"\"\"This will change the answer to preprocessed version\n",
    "    \"\"\"\n",
    "    occurence = {}\n",
    "\n",
    "    for ans_entry in answers_dset:\n",
    "        answers = ans_entry['answers']\n",
    "        gtruth = ans_entry['multiple_choice_answer']\n",
    "        gtruth = preprocess_answer(gtruth)\n",
    "        if gtruth not in occurence:\n",
    "            occurence[gtruth] = set()\n",
    "        occurence[gtruth].add(ans_entry['question_id'])\n",
    "    all_keys = list(occurence.keys())\n",
    "    for i in range(len(all_keys)):\n",
    "        answer = all_keys[i]\n",
    "        if len(occurence[answer]) < min_occurence:\n",
    "            occurence.pop(answer)\n",
    "\n",
    "    print('Num of answers that appear >= %d times: %d' % (\n",
    "        min_occurence, len(occurence)))\n",
    "    return occurence\n",
    "\n",
    "\n",
    "def create_ans2label(occurence, name, cache_root='data/cache'):\n",
    "    \"\"\"Note that this will also create label2ans.pkl at the same time\n",
    "    occurence: dict {answer -> whatever}\n",
    "    name: prefix of the output file\n",
    "    cache_root: str\n",
    "    \"\"\"\n",
    "    ans2label = {}\n",
    "    label2ans = []\n",
    "    label = 0\n",
    "    for answer in occurence:\n",
    "        label2ans.append(answer)\n",
    "        ans2label[answer] = label\n",
    "        label += 1\n",
    "\n",
    "    utils.create_dir(cache_root)\n",
    "\n",
    "    cache_file = os.path.join(cache_root, name+'_ans2label.pkl')\n",
    "    cPickle.dump(ans2label, open(cache_file, 'wb'))\n",
    "    cache_file = os.path.join(cache_root, name+'_label2ans.pkl')\n",
    "    cPickle.dump(label2ans, open(cache_file, 'wb'))\n",
    "    return ans2label\n",
    "\n",
    "\n",
    "def compute_target(answers_dset, ans2label, name, cache_root='data/cache'):\n",
    "    \"\"\"Augment answers_dset with soft score as label\n",
    "    ***answers_dset should be preprocessed***\n",
    "    Write result into a cache file\n",
    "    \"\"\"\n",
    "    target = []\n",
    "    for ans_entry in answers_dset:\n",
    "        answers = ans_entry['answers']\n",
    "        answer_count = {}\n",
    "        for answer in answers:\n",
    "            answer_ = answer['answer']\n",
    "            answer_count[answer_] = answer_count.get(answer_, 0) + 1\n",
    "\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for answer in answer_count:\n",
    "            if answer not in ans2label:\n",
    "                continue\n",
    "            labels.append(ans2label[answer])\n",
    "            score = get_score(answer_count[answer])\n",
    "            scores.append(score)\n",
    "\n",
    "        target.append({\n",
    "            'question_id': ans_entry['question_id'],\n",
    "            'image_id': ans_entry['image_id'],\n",
    "            'labels': labels,\n",
    "            'scores': scores\n",
    "        })\n",
    "\n",
    "    utils.create_dir(cache_root)\n",
    "    cache_file = os.path.join(cache_root, name+'_target.pkl')\n",
    "    cPickle.dump(target, open(cache_file, 'wb'))\n",
    "    return target\n",
    "\n",
    "\n",
    "def get_answer(qid, answers):\n",
    "    for ans in answers:\n",
    "        if ans['question_id'] == qid:\n",
    "            return ans\n",
    "\n",
    "\n",
    "def get_question(qid, questions):\n",
    "    for question in questions:\n",
    "        if question['question_id'] == qid:\n",
    "            return question\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_answer_file = root_dir + 'v2_mscoco_train2014_annotations.json'\n",
    "    train_answers = json.load(open(train_answer_file))['annotations']\n",
    "\n",
    "    val_answer_file = root_dir + 'v2_mscoco_val2014_annotations.json'\n",
    "    val_answers = json.load(open(val_answer_file))['annotations']\n",
    "\n",
    "    train_question_file = root_dir + 'v2_OpenEnded_mscoco_train2014_questions.json'\n",
    "    train_questions = json.load(open(train_question_file))['questions']\n",
    "\n",
    "    val_question_file = root_dir + 'v2_OpenEnded_mscoco_val2014_questions.json'\n",
    "    val_questions = json.load(open(val_question_file))['questions']\n",
    "\n",
    "    answers = train_answers + val_answers\n",
    "    occurence = filter_answers(answers, 9)\n",
    "    ans2label = create_ans2label(occurence, 'trainval')\n",
    "    compute_target(train_answers, ans2label, 'train')\n",
    "    compute_target(val_answers, ans2label, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detection_features_converter to convert raw image features into hdf5 and pkl formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading tsv...\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "iterator should return strings, not bytes (did you open the file in text mode?)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e2a0a95f8f41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtsv_in_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsv_in_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfieldnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFIELDNAMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_boxes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_boxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mimage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/csv.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# Used only for its side effect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: iterator should return strings, not bytes (did you open the file in text mode?)"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "#sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "import base64\n",
    "import csv\n",
    "import h5py\n",
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "FIELDNAMES = ['image_id', 'image_w', 'image_h', 'num_boxes', 'boxes', 'features']\n",
    "infile = 'data/trainval_36/trainval_resnet101_faster_rcnn_genome_36.tsv'\n",
    "train_data_file = 'data/train36.hdf5'\n",
    "val_data_file = 'data/val36.hdf5'\n",
    "train_indices_file = 'data/train36_imgid2idx.pkl'\n",
    "val_indices_file = 'data/val36_imgid2idx.pkl'\n",
    "train_ids_file = 'data/train_ids.pkl'\n",
    "val_ids_file = 'data/val_ids.pkl'\n",
    "\n",
    "feature_length = 2048\n",
    "num_fixed_boxes = 36\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    h_train = h5py.File(train_data_file, \"w\")\n",
    "    h_val = h5py.File(val_data_file, \"w\")\n",
    "\n",
    "    if os.path.exists(train_ids_file) and os.path.exists(val_ids_file):\n",
    "        train_imgids = cPickle.load(open(train_ids_file))\n",
    "        val_imgids = cPickle.load(open(val_ids_file))\n",
    "    else:\n",
    "        train_imgids = utils.load_imageid(root_dir+'train2014')\n",
    "        val_imgids = utils.load_imageid(root_dir+'val2014')\n",
    "        cPickle.dump(train_imgids, open(train_ids_file, 'wb'))\n",
    "        cPickle.dump(val_imgids, open(val_ids_file, 'wb'))\n",
    "\n",
    "    train_indices = {}\n",
    "    val_indices = {}\n",
    "\n",
    "    train_img_features = h_train.create_dataset(\n",
    "        'image_features', (len(train_imgids), num_fixed_boxes, feature_length), 'f')\n",
    "    train_img_bb = h_train.create_dataset(\n",
    "        'image_bb', (len(train_imgids), num_fixed_boxes, 4), 'f')\n",
    "    train_spatial_img_features = h_train.create_dataset(\n",
    "        'spatial_features', (len(train_imgids), num_fixed_boxes, 6), 'f')\n",
    "\n",
    "    val_img_bb = h_val.create_dataset(\n",
    "        'image_bb', (len(val_imgids), num_fixed_boxes, 4), 'f')\n",
    "    val_img_features = h_val.create_dataset(\n",
    "        'image_features', (len(val_imgids), num_fixed_boxes, feature_length), 'f')\n",
    "    val_spatial_img_features = h_val.create_dataset(\n",
    "        'spatial_features', (len(val_imgids), num_fixed_boxes, 6), 'f')\n",
    "\n",
    "    train_counter = 0\n",
    "    val_counter = 0\n",
    "\n",
    "    print(\"reading tsv...\")\n",
    "    with open(infile, \"r+b\") as tsv_in_file:\n",
    "        reader = csv.DictReader(tsv_in_file, delimiter='\\t', fieldnames=FIELDNAMES)\n",
    "        for item in reader:\n",
    "            item['num_boxes'] = int(item['num_boxes'])\n",
    "            image_id = int(item['image_id'])\n",
    "            image_w = float(item['image_w'])\n",
    "            image_h = float(item['image_h'])\n",
    "            bboxes = np.frombuffer(\n",
    "                base64.decodestring(item['boxes']),\n",
    "                dtype=np.float32).reshape((item['num_boxes'], -1))\n",
    "\n",
    "            box_width = bboxes[:, 2] - bboxes[:, 0]\n",
    "            box_height = bboxes[:, 3] - bboxes[:, 1]\n",
    "            scaled_width = box_width / image_w\n",
    "            scaled_height = box_height / image_h\n",
    "            scaled_x = bboxes[:, 0] / image_w\n",
    "            scaled_y = bboxes[:, 1] / image_h\n",
    "\n",
    "            box_width = box_width[..., np.newaxis]\n",
    "            box_height = box_height[..., np.newaxis]\n",
    "            scaled_width = scaled_width[..., np.newaxis]\n",
    "            scaled_height = scaled_height[..., np.newaxis]\n",
    "            scaled_x = scaled_x[..., np.newaxis]\n",
    "            scaled_y = scaled_y[..., np.newaxis]\n",
    "\n",
    "            spatial_features = np.concatenate(\n",
    "                (scaled_x,\n",
    "                 scaled_y,\n",
    "                 scaled_x + scaled_width,\n",
    "                 scaled_y + scaled_height,\n",
    "                 scaled_width,\n",
    "                 scaled_height),\n",
    "                axis=1)\n",
    "\n",
    "            if image_id in train_imgids:\n",
    "                train_imgids.remove(image_id)\n",
    "                train_indices[image_id] = train_counter\n",
    "                train_img_bb[train_counter, :, :] = bboxes\n",
    "                train_img_features[train_counter, :, :] = np.frombuffer(\n",
    "                    base64.decodestring(item['features']),\n",
    "                    dtype=np.float32).reshape((item['num_boxes'], -1))\n",
    "                train_spatial_img_features[train_counter, :, :] = spatial_features\n",
    "                train_counter += 1\n",
    "            elif image_id in val_imgids:\n",
    "                val_imgids.remove(image_id)\n",
    "                val_indices[image_id] = val_counter\n",
    "                val_img_bb[val_counter, :, :] = bboxes\n",
    "                val_img_features[val_counter, :, :] = np.frombuffer(\n",
    "                    base64.decodestring(item['features']),\n",
    "                    dtype=np.float32).reshape((item['num_boxes'], -1))\n",
    "                val_spatial_img_features[val_counter, :, :] = spatial_features\n",
    "                val_counter += 1\n",
    "            else:\n",
    "                assert False, 'Unknown image id: %d' % image_id\n",
    "\n",
    "    if len(train_imgids) != 0:\n",
    "        print('Warning: train_image_ids is not empty')\n",
    "\n",
    "    if len(val_imgids) != 0:\n",
    "        print('Warning: val_image_ids is not empty')\n",
    "\n",
    "    cPickle.dump(train_indices, open(train_indices_file, 'wb'))\n",
    "    cPickle.dump(val_indices, open(val_indices_file, 'wb'))\n",
    "    h_train.close()\n",
    "    h_val.close()\n",
    "    print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training the model and save the model under 'saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def parse_args():\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument('--epochs', type=int, default=30)\\n    parser.add_argument('--num_hid', type=int, default=1024)\\n    parser.add_argument('--model', type=str, default='baseline0_newatt')\\n    parser.add_argument('--output', type=str, default='saved_models/exp0')\\n    parser.add_argument('--batch_size', type=int, default=512)\\n    parser.add_argument('--seed', type=int, default=1111, help='random seed')\\n    args = parser.parse_args()\\n    return args\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from dataset import Dictionary, VQAFeatureDataset\n",
    "import base_model\n",
    "from train import train\n",
    "import utils\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    seed = 1111\n",
    "    batch_size = 512\n",
    "    model = 'baseline0'\n",
    "    epochs = 30\n",
    "    num_hid = 1024\n",
    "    output = './saved_models/exp0'\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    dictionary = Dictionary.load_from_file('data/dictionary.pkl')\n",
    "    train_dset = VQAFeatureDataset('train', dictionary)\n",
    "    eval_dset = VQAFeatureDataset('val', dictionary)\n",
    "    batch_size = batch_size\n",
    "\n",
    "    constructor = 'build_%s' % model\n",
    "    model = getattr(base_model, constructor)(train_dset, num_hid).cuda()\n",
    "    model.w_emb.init_embedding('data/glove6b_init_300d.npy')\n",
    "\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "\n",
    "    train_loader = DataLoader(train_dset, batch_size, shuffle=True, num_workers=1)\n",
    "    eval_loader =  DataLoader(eval_dset, batch_size, shuffle=True, num_workers=1)\n",
    "    train(model, train_loader, eval_loader, epochs, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
